{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/main/HW5/HW_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "t848-F03nCxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HW5: Neural Sequence Labeling\n",
        "\n",
        "For this assignment, we'll provide an implementation of BERT and view its performance on **named entity recognition (NER)**. We use a standard NER dataset that adheres to the BIO tagging format discussed in lecture.\n",
        "\n",
        "We ask you to implement:\n",
        "\n",
        "1) a span extraction method, to determine which entities are present in a given piece of text\n",
        "\n",
        "2) a method to calculate F1, which evaluates our model's performance. "
      ],
      "metadata": {
        "id": "VZQvyvkbNbIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure Transformers is installed \n",
        "# https://huggingface.co/docs/transformers/index\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Q6viYVn8znfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6ToJtg5zZBV"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "aMIanCLEziKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro: Wordpiece Tokenization Exploration\n",
        "\n",
        "To start the notebook, let's explore how BERT tokenizes inputs. There are no deliverables for this portion, but we encourage you to step through these cells to understand what happens when words are inputted into BERT!\n",
        "\n",
        "BERT uses **WordPiece** tokenization, which is a subword tokenzation technique that breaks down words that don't appear within its 30K-word vocabulary into small pieces. The word \"vaccinated\", for instance, is tokenized as `[\"va\", \"##cci\", \"##nated\"]`\n",
        "\n",
        "To explore how BERT tokenizes inputs, let's first load in the [Bert-Base](https://github.com/google-research/bert#bert) model from the [Transformers](https://huggingface.co/docs/transformers/model_doc/bert) library. This call allows us to use a pretrained BERT model (i.e., one that has already gone through the training phase of MLM + NSP discussed in the [2/15 lecture](https://people.ischool.berkeley.edu/~dbamman/nlp22_slides/9_LM_3.pdf)) and use it for whatever tasks we'd like."
      ],
      "metadata": {
        "id": "odVbbdepNmyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = BertModel.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "t0ntR1SFNrJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with an example sentence:\n",
        "\n",
        "*New data shows 26 states have fully vaccinated more than half their residents.*\n",
        "\n",
        "and see how BERT tokenizes it."
      ],
      "metadata": {
        "id": "Wi5C7EKcN-yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=tokenizer(\"New data shows 26 states have fully vaccinated more than half their residents.\", return_tensors=\"pt\")\n",
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "P7SrILBAOC9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how common words like \"have\" are represented in their entirety, while \"vaccinated\" is broken into 3 different parts. You can also see the reserved start `[CLS]` and ending `[SEP]` tags we discussed in class. BERT will generate representations of each WordPiece token, including these special `[CLS]` and `[SEP]` tags.\n",
        "\n",
        "You can really see the effect of WordPiece when you provide a word that BERT, most likely, never encountered during its training, like *supercalifragilisticexpialidocious*"
      ],
      "metadata": {
        "id": "PXs4bK8kOUNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=tokenizer(\"BERT is supercalifragilisticexpialidocious\", return_tensors=\"pt\")\n",
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "wnLipoNDOZO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's work with an example sentence, *This jam is delicious*"
      ],
      "metadata": {
        "id": "m3-QNX34OqTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=tokenizer(\"This jam is delicious\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "coimEF6qOl8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representations for each of BERT layers (12 in this model) are accessible, but let's explore just the outputs from the final layer.  This BERT model has 768-dimensional representations, so this 6-token input (`[CLS, this, jam, is, delicious, [SEP]`) has an output that is is a 1 x 6 tokens x 768 dimensional tensor."
      ],
      "metadata": {
        "id": "G02ivgctOhar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_hidden_states = outputs.last_hidden_state\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "YNMx4kg7O1sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we move to using BERT to help us carry out NER (which is, fundamentally, a classification problem), what can we do with just these representations?  \n",
        "\n",
        "While we used word2vec-style static embeddings to find nearest neighbors for word *types*, we can do the same here for word *tokens*."
      ],
      "metadata": {
        "id": "ZnnSdMtTO6eC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this should look familiar!\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
      ],
      "metadata": {
        "id": "QU4jJktdO9u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how BERT does at answering questions of ambiguity, which is of major concern when carrying out a task like NER (as well as other topics we'll explore later in class like word senses)\n",
        "\n",
        "We'll start with a base sentence:\n",
        "\n",
        "*I ate some jam with toast*\n",
        "\n",
        "that uses the word *jam* in the sense of what you spread onto toast, i.e.\n",
        "\"A conserve of fruit prepared by boiling it with sugar to a pulp.\" ([OED](https://www.oed.com/view/Entry/100680?rskey=v7CKhW&result=2#eid)).\n",
        "\n",
        "Now we'll write a method that extracts the representation BERT uses for a given word in a sentence."
      ],
      "metadata": {
        "id": "7ikiO-7KPAHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_for_token(string, term):\n",
        "    \n",
        "    # tokenize\n",
        "    inputs = tokenizer(string, return_tensors=\"pt\")\n",
        "    \n",
        "    # convert input ids to words\n",
        "    tokens=tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    \n",
        "    # find the first location of the query term among those tokens (so we know which BERT rep to use)\n",
        "    term_idx=tokens.index(term)\n",
        "    \n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # return the BERT rep for that token index\n",
        "    # The output is a pytorch tensor object, but let's convert it to a numpy object to work with numpy functions\n",
        "    \n",
        "    return outputs.last_hidden_state[0][term_idx].detach().numpy()"
      ],
      "metadata": {
        "id": "oxMYA0cEPFSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start with some generic methods for reading the data our model will apply BIO tags to. We will be working with the 2003 CoNLL NER dataset [(Sang and Kim, 2003)](https://arxiv.org/pdf/cs/0306050v1.pdf)."
      ],
      "metadata": {
        "id": "hKbahhDcRxqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"I ate some jam with toast\"\n",
        "\n",
        "query_rep=get_bert_for_token(query, \"jam\")\n",
        "# note the shape\n",
        "print(query_rep.shape)"
      ],
      "metadata": {
        "id": "vnN1opxnPIws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's write sentences using other forms of jam, like the [sense](https://www.oed.com/view/Entry/100679?rskey=v7CKhW&result=1#eid) that refers to \"preventing movements\" and then compare how closely their usage of the word jam matches our starter sentence (according to BERT).\n",
        "\n"
      ],
      "metadata": {
        "id": "wFlwVgJoPNNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comp_sents=[\"She got me out of a real jam\", \"This jam is made of strawberries\", \"I sat in a traffic jam for 2 hours\", \"The Grateful Dead used to jam for like two days straight.\", \"My grandma makes the best jam.\", \"I had to jam on the brakes to avoid hitting him.\"]"
      ],
      "metadata": {
        "id": "l7ukm7u2PUwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vals=[]\n",
        "for sent in comp_sents:\n",
        "    comp_rep=get_bert_for_token(sent, \"jam\")\n",
        "    cos_sim=cosine_similarity(query_rep, comp_rep)\n",
        "    vals.append((cos_sim, query, sent))\n",
        "\n",
        "for c, q, s in reversed(sorted(vals)):\n",
        "    print(\"%.3f\\t%s\\t%s\" % (c, q, s))"
      ],
      "metadata": {
        "id": "V4W_reM6PXYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've explored how BERT tokenizes data firsthand, let's incorporate our learning task and see how a BERT model performs at tagging named entities in a given text.\n",
        "\n",
        "## Neural NER\n",
        "\n",
        "We'll start with some generic methods for reading the data our model will apply BIO tags to. We will be working with the 2003 CoNLL NER dataset [(Sang and Kim, 2003)](https://arxiv.org/pdf/cs/0306050v1.pdf)."
      ],
      "metadata": {
        "id": "b6LkNnZRPZ5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_labels(filename):\n",
        "\tlabels={}\n",
        "\twith open(filename) as file:\n",
        "\t\tfor line in file:\n",
        "\t\t\tcols=line.rstrip().split(\"\\t\")\n",
        "\t\t\tif len(cols) < 2:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tlabel=cols[1]\n",
        "\t\t\tif label not in labels:\n",
        "\t\t\t\tlabels[label]=len(labels)\n",
        "\t\n",
        "\treturn labels\n",
        "\n",
        "def read_data(filename, labels):\n",
        "\tsentences=[]\n",
        "\tsentence=[]\n",
        "\twith open(filename) as file:\n",
        "\t\tfor line in file:\n",
        "\t\t\tcols=line.rstrip().split(\"\\t\")\n",
        "\t\t\tif len(cols) < 2:\n",
        "\t\t\t\tif len(sentence) > 0:\n",
        "\t\t\t\t\tsentences.append(sentence)\n",
        "\t\t\t\t\tsentence=[]\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\ttoken=cols[0]\n",
        "\t\t\t\tassert cols[1] in labels\n",
        "\t\t\t\tlabel=labels[cols[1]]\n",
        "\t\t\t\tsentence.append((token, label))\n",
        "\t\n",
        "\tif len(sentence) > 0:\n",
        "\t\tsentences.append(sentence)\n",
        "\t\tsentence=[]\n",
        "\n",
        "\treturn sentences"
      ],
      "metadata": {
        "id": "tfKERBqQ1WhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW5/data/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW5/data/valid.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW5/data/test.txt\n",
        "\n",
        "labels=read_labels(\"train.txt\")\n",
        "rev_labels={labels[key]:key for key in labels}\n",
        "\n",
        "train=read_data(\"train.txt\", labels)\n",
        "dev=read_data(\"valid.txt\", labels)\n",
        "test=read_data(\"test.txt\", labels)"
      ],
      "metadata": {
        "id": "Da7t9hICwLqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTClassifier\n",
        "Then, let's write out a BERT classifier in Pytorch so you can see the model we'll be using for NER tagging."
      ],
      "metadata": {
        "id": "CLbdd491RXSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "\n",
        "\tdef __init__(self, params):\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.model_name=params[\"model_name\"]\n",
        "\t\tself.do_lower_case = params[\"doLowerCase\"]\n",
        "\n",
        "\t\tself.tokenizer = BertTokenizer.from_pretrained(self.model_name, do_lower_case=params[\"doLowerCase\"], do_basic_tokenize=False)\n",
        "\t\tself.bert = BertModel.from_pretrained(self.model_name)\n",
        "\t\n",
        "\t\tself.num_labels = params[\"label_length\"]\n",
        "\t\tself.fc = nn.Linear(params[\"embedding_size\"], self.num_labels)\n",
        "\t\n",
        "\t\tself.device=device\n",
        "\n",
        "\tdef get_batches(self, data, batch_size=32):\n",
        "\n",
        "\t\tbatches_original=[]\n",
        "\t\tbatches_x=[]\n",
        "\t\tbatches_y=[]\n",
        "\t\tbatches_attention=[]\n",
        "\t\t\n",
        "\t\tfor i in range(0, len(data), batch_size):\n",
        "\n",
        "\t\t\tcurrent_x=[]\n",
        "\t\t\tcurrent_y=[]\n",
        "\t\t\tcurrent_o=[]\n",
        "\n",
        "\t\t\tfor sentence in data[i:i+batch_size]:\n",
        "\t\t\t\twp_sentence=[self.tokenizer.convert_tokens_to_ids(\"[CLS]\")]\n",
        "\t\t\t\twp_labels=[-100]\n",
        "\n",
        "\t\t\t\tfor token, label in sentence:\n",
        "\n",
        "\t\t\t\t\tif self.do_lower_case:\n",
        "\t\t\t\t\t\ttoken=token.lower()\n",
        "\t\t\t\t\t\t\n",
        "\t\t\t\t\twp_tokens=self.tokenizer.convert_tokens_to_ids(self.tokenizer.tokenize(token))\n",
        "\t\t\t\t\twp_sentence.extend(wp_tokens)\n",
        "\t\t\t\t\twp_labels.append(label)\n",
        "\t\t\t\t\tfor wp_tok in wp_tokens[1:]:\n",
        "\t\t\t\t\t\twp_labels.append(-100)\n",
        "\n",
        "\t\t\t\t\twp_sentence.append(self.tokenizer.convert_tokens_to_ids(\"[SEP]\"))\n",
        "\t\t\t\t\twp_labels.append(-100)\n",
        "\n",
        "\t\t\t\tif len(wp_sentence) >= 512:\n",
        "\t\t\t\t\tprint(\"sentence is longer than BERT's 512 wp token max: %s\" % len(wp_sentence))\n",
        "\t\t\t\t\tsys.exit(1)\n",
        "\n",
        "\t\t\t\tcurrent_x.append(wp_sentence)\n",
        "\t\t\t\tcurrent_y.append(wp_labels)\n",
        "\t\t\t\t\n",
        "\t\t\t\twords=[x[0] for x in sentence]\n",
        "\t\t\t\tcurrent_o.append(words)\n",
        "\n",
        "\t\t\t# batch each sentence to the max size within that batch\n",
        "\t\t\tmax_len=max([len(x) for x in current_x])\n",
        "\t\t\tattention_mask=np.ones((len(current_x), max_len))\n",
        "\t\t\tfor idx in range(len(current_x)):\n",
        "\t\t\t\tfor i in range(len(current_x[idx]), max_len):\n",
        "\t\t\t\t\tcurrent_x[idx].append(0)\n",
        "\t\t\t\t\tcurrent_y[idx].append(-100)\n",
        "\t\t\t\t\tattention_mask[idx][i]=0\n",
        "\n",
        "\t\t\tbatches_original.append(current_o)\n",
        "\t\t\tbatches_x.append(torch.LongTensor(current_x))\n",
        "\t\t\tbatches_y.append(torch.LongTensor(current_y))\n",
        "\t\t\tbatches_attention.append(torch.LongTensor(attention_mask))\n",
        "\t\t\t\n",
        "\t\t# each sentence in each batch has:\n",
        "\t\t# -- word piece token ids (batches_x)\n",
        "\t\t# -- attention mask (noting which tokens are just padding)\n",
        "\t\t# -- NER labels (one per token\n",
        "\t\t# -- original (i.e., non-word piece) words\n",
        "\t\treturn batches_x, batches_attention, batches_y, batches_original\n",
        "  \n",
        "\n",
        "\tdef forward(self, input_ids, attention_mask): \n",
        "\t\n",
        "\t\tinput_ids=input_ids.to(self.device)\n",
        "\t\tattention_mask=attention_mask.to(self.device)\n",
        "\t\t\t\n",
        "\t\toutput = self.bert(input_ids=input_ids,\n",
        "\t\t\t\t\t\t attention_mask=attention_mask,\n",
        "\t\t\t\t\t\t output_hidden_states=True)\n",
        "\n",
        "\t\thidden_states=output[\"hidden_states\"]\n",
        "\t\tout=hidden_states[-1]\n",
        "\n",
        "\t\tlogits = self.fc(out)\n",
        "\n",
        "\t\treturn logits"
      ],
      "metadata": {
        "id": "HSqr6XJM2buH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using BERT for NER\n",
        "\n",
        "(Note: No deliverables are in this section; it's optional).\n",
        "\n",
        "These methods are good examples of how to train a language model for a task of your choosing and carry out predictions on unseen input.\n",
        "\n",
        "`train_and_evaluate` takes in various settings for a BERT model (like how large the embeddings should be, whether the model should lowercase all the inputs, etc.) instantiates a `BERTClassifier` and trains the model for our NER task. You'll see that it actually calls the `get_span_f1` method you are writing for this assignment. This provides a gauge for how well the model is performing NER. Also, this method will continually [save](https://pytorch.org/tutorials/beginner/saving_loading_models.html) whichever iteration of the model performed the best, and it ultimately returns that best model. \n",
        "\n",
        "`predict` generates predictions (applies BIO tags) to unseen data. For each sentence it tags, it returns the predicted BIO tag alongside the correct classification. \n"
      ],
      "metadata": {
        "id": "xGmeCa_gWKaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(bert_model_name, model_filename, embedding_size, num_epochs, doLowerCase =None):\n",
        "\n",
        "  bert_model = BERTClassifier(params={\"doLowerCase\": doLowerCase, \"model_name\": bert_model_name, \"embedding_size\":embedding_size, \"label_length\": len(labels)})\n",
        "  bert_model.to(device)\n",
        "\n",
        "  train_batch_x, train_attention, train_batch_y, train_original = bert_model.get_batches(train)\n",
        "  dev_batch_x, dev_attention, dev_batch_y, dev_original = bert_model.get_batches(dev)\n",
        "\n",
        "  optimizer = torch.optim.Adam(bert_model.parameters(), lr=1e-5)\n",
        "  cross_entropy=nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "  bestF1 = 0.\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch', epoch + 1)\n",
        "    \n",
        "    # Train\n",
        "    bert_model.train()\n",
        "\n",
        "    for x, a, y in tqdm.notebook.tqdm(list(zip(train_batch_x, train_attention, train_batch_y))):\n",
        "      y=y.to(device)\n",
        "      y_pred = bert_model.forward(x, a)\n",
        "      loss = cross_entropy(y_pred.view(-1, bert_model.num_labels), y.view(-1))\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    # Evaluate\n",
        "    dev_predictions=predict(bert_model, dev_batch_x, dev_attention, dev_batch_y, dev_original)\n",
        "\n",
        "    f1=get_span_f1(dev_predictions)\n",
        "\n",
        "    print(\"span F1: %.3f\" % f1)\n",
        "    if f1 > bestF1:\n",
        "      print(\"%.3f is better than %.3f, saving ...\"  % (f1, bestF1))\n",
        "      torch.save(bert_model.state_dict(), model_filename)\n",
        "      bestF1 = f1\n",
        "\n",
        "  bert_model.load_state_dict(torch.load(model_filename))\n",
        "\n",
        "  return bert_model\n",
        "\n",
        "def predict(model, batch_x, attention, batch_y, original):\n",
        "    model.eval()\n",
        "    dev_predictions=[]\n",
        "    corrected_ill_formed=0\n",
        "\n",
        "    for x, a, y, o in zip(batch_x, attention, batch_y, original):\n",
        "      y=y.to(device)\n",
        "      y_pred = model.forward(x, a)\n",
        "      size=y_pred.shape\n",
        "      y_pred=y_pred.detach().cpu()\n",
        "\n",
        "      for sentence in range(size[0]):\n",
        "        sentence_preds=[]\n",
        "        o_indx=0\n",
        "        open_cat=None\n",
        "\n",
        "        # start at token index 1 to skip [CLS] token\n",
        "        for token in range(1,size[1]):\n",
        "          # ignore CLS, SEP, padding, and all but the first WP token (all marked with a label of -100)\n",
        "          if y[sentence][token] != -100:\n",
        "            pred=int(np.argmax(y_pred[sentence][token]))\n",
        "            true_label=rev_labels[int(y[sentence][token])]\n",
        "            pred_label=rev_labels[pred]\n",
        "\n",
        "            # if an I- label is not preceded by a B- label of the same category, change that I- to a B-.\n",
        "            label_parts=pred_label.split(\"-\")\n",
        "            if len(label_parts) == 2:\n",
        "              bio=label_parts[0]\n",
        "              cat=label_parts[1]\n",
        "\n",
        "              # other small corrections for ill-formed tokens\n",
        "              if bio == \"I\" and open_cat != cat:\n",
        "                pred_label=\"B-%s\" % cat\n",
        "                open_cat=cat\n",
        "\n",
        "              if bio == \"B\":\n",
        "                open_cat=cat\n",
        "            \n",
        "            sentence_preds.append((o[sentence][o_indx], pred_label, true_label))\n",
        "            o_indx+=1\n",
        "        dev_predictions.append(sentence_preds)\n",
        "      return dev_predictions"
      ],
      "metadata": {
        "id": "PCkMzLcezYUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you'll work on two methods that will help us evaluate BERT's performance at NER.\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "MGZQWIqgXqNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deliverable 1 - `get_spans()`\n",
        "\n",
        "As input, this method will take in a list of strings in BIO notation. The method should parse each relevant entity.\n",
        "\n",
        "As output, return the entities with their spand boundaries. \n",
        "\n",
        "See the assignment .pdf for a detailed explanation of BIO notation.\n",
        "\n",
        "Example input: `[\"O\", \"B-PER\", \"I-PER\", \"O\", \"B-PER\", \"B-LOC\", \"I-LOC\", \"I-LOC\", \"O\"]`\n",
        "\n",
        "Example output: `{ (1,2,PER), (4,4,PER), (5,7,LOC) }`"
      ],
      "metadata": {
        "id": "-TAlKcnD-7cP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spans(token_label_list):\n",
        "\t\"\"\"Return a set of the spans entailed by the BIO tags.\n",
        "\t\n",
        "\tArgs:\n",
        "\t\t\ttoken_label_list: a list of string BIO tags\n",
        "\n",
        "\tHints:\n",
        "\t- each string in the list can be split into its BIO tag (B, I, or O) and its category (if applicable)\n",
        "\t- every \"new\" entity starts with B.\n",
        "\t- the same entity can be entailed by multiple tags, and that entity terminates once another B or O tag appears. \n",
        "\t- use the spans dict to track where an entity begins, ends, and its category\n",
        "\t- an entity of span \"length\" 1 should have bounds like (2,2); \"length\" 2 should have bounds like (2,3), and so on\n",
        "\n",
        "\t\"\"\"\n",
        "\tspans={}\n",
        "\tstart=None\n",
        "\t\n",
        "\t# BEGIN SOLUTION \n",
        "\t\n",
        " \t# END SOLUTION"
      ],
      "metadata": {
        "id": "VbGevWPY4g-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deliverable 2 - `get_span_f1()`\n",
        "\n",
        "As input, this method will take in `predictions`, a list containing every sentence tagged by the model. The sentences themselves are lists of (token, predicted label, true label) triples. `predictions` could potentially hold hundreds of entries, since this method calculates the overall F1 score for our model. So be sure that your method works with more than single sentence inputted.\n",
        "\n",
        "As output, this method should return the overall F1 score (a decimal number) for the entire model.\n",
        "\n",
        "See the assignment .pdf for a detailed explanation of F1.\n",
        "\n",
        "Example input:\n",
        "\n",
        "```python\n",
        "predictions = []\n",
        "predictions.append([('Tim', 'B-PER', 'B-PER'), ('Cook', 'I-PER', 'I-PER'), ('is', 'O', 'O'), ('the', 'O', 'O'), ('CEO', 'O', 'O'), ('of', 'O', 'O'), ('Apple', 'O', 'B-ORG')])\n",
        "predictions.append([('He', 'O', 'O'), ('started', 'O', 'O'), ('in', 'O', 'O'), ('2011', 'O', 'O')])\n",
        "get_span_f1(predictions)\n",
        "```\n",
        "\n",
        "Example output: 0.667\n",
        "\n",
        "Why? We correctly identified 1 of the 2 true spans (recall of 1/2) and\n",
        "our 1 prediction was correct (precision of 1)."
      ],
      "metadata": {
        "id": "FxJ8n57u_Ckj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_span_f1(predictions):\n",
        "\t\"\"\"Return span F1 Score for predicted BIO tags\n",
        "\t\t\t\n",
        "\t\t\tArgs:\n",
        "\t\t\t\t\tpredictions, a list of sentences w/ their BIO tag predictions (from the model) and gold standard labels\n",
        "\n",
        "\t\t\tHints:\n",
        "\t\t\t\t- every sentence contains a token with its predicted label and true label\n",
        "\t\t\t\t- recall that your get_spans method extracts spans from a list of BIO-tag labels\n",
        "\t\t\t\t- use set intersection to find the number of matches between the predicted spans and true spans\n",
        "\t\t\t\t- avoid division by 0!\n",
        "\t\t\t\t- this metric needs to be evaluated for the entire model, so only calculate precision, recall and then F1 once\n",
        "\n",
        "\t\"\"\"\n",
        "\n",
        "\t# BEGIN SOLUTION \n",
        "\n",
        "\t# END SOLUTION"
      ],
      "metadata": {
        "id": "TYK4zPpN4mm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Further Exploration\n",
        "\n",
        "(Note: No deliverables are in this section; it's optional).\n",
        "\n",
        "To see how you can use your span F1 measure during training, call this to invoke the functions you wrote for deliverables 1 and 2. \n",
        "\n",
        "First, let's use a smaller version of BERT that sacrifices accuracy for speed of training, so you should see results within five minutes. Improvement should be rapid, and after 10 epochs your F1 should be around .6"
      ],
      "metadata": {
        "id": "knbF4fc16w_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert Tiny - 2 layers, 128 dimensional embeddings, doLowerCase = True\n",
        "ner_f1_bert_tiny_uncased_model=train_and_evaluate(\"google/bert_uncased_L-2_H-128_A-2\", \"ner-f1-bert-tiny-uncased\", embedding_size=128, num_epochs = 10, doLowerCase=True)"
      ],
      "metadata": {
        "id": "DqfzTgrOPLX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use that trained model to make predictions about new sentences."
      ],
      "metadata": {
        "id": "slLJGhm_6dTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentence(model, sentence):\n",
        "\n",
        "  toks=nltk.word_tokenize(sentence)\n",
        "  input=[[(word, 0) for word in toks]]\n",
        "  predict_batch_x, predict_attention, predict_batch_y, predict_original=ner_f1_bert_tiny_uncased_model.get_batches(input)\n",
        "  dev_predictions=predict(model, predict_batch_x, predict_attention, predict_batch_y, predict_original)\n",
        "  for sent in dev_predictions:\n",
        "    for tok, pred, _ in sent:\n",
        "      print(tok, pred)"
      ],
      "metadata": {
        "id": "4pkpUT-yyumG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_sentence(ner_f1_bert_tiny_uncased_model, \"John is from Washington, DC\")"
      ],
      "metadata": {
        "id": "Go-SB93y6B0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Improvements\n",
        "\n",
        "Google has released a number of smaller BERT models with fewer layers (2, 4, 6, 8, 10) and smaller dimensions (128, 256, 512) that effectively trade off accuracy for speed. For the prior portions of the assignment, we used a model with only 2 neural layers and an embedding size of 512. Plus, that model was uncased (so all text is lowercase).\n",
        "\n",
        "The size and parameters of model we use for this task will no doubt affect our performance. Try experimenting with a larger BERT model and see how much performance improves (relative, of course, to other important factors like learning time!). \n",
        "\n",
        "To use these models in the `transformers` library that we have been using, the correct name of the model can be derived from the URL linking to it:\n",
        "\n",
        "https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip -> `google/bert_uncased_L-2_H-128_A-2`\n",
        "\n",
        "All of the smaller models are uncased (so all text is lowercase), so be sure to set `doLowerCase` to be true if needed. You'll also need to change the embedding_size parameter to this function based on the H value from the model (listed both on the BERT [Github page](https://github.com/google-research/bert#bert) and in the model's URL). One sample model is provided below.\n",
        "\n"
      ],
      "metadata": {
        "id": "QQyu3sVQJIX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert Medium - 8 layers, 512 dimensional embeddings, doLowerCase = True\n",
        "ner_f1_bert_medium_uncased_model=train_and_evaluate(\"google/bert_uncased_L-8_H-512_A-8\", \"ner-f1-bert-medium-uncased\", embedding_size=512, num_epochs = 5, doLowerCase=True)"
      ],
      "metadata": {
        "id": "pVyls6mU0C9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert BASE - 12 layers, 768 dimensional embeddings, doLowerCase = True\n",
        "# This is named differently because it is the \"standard\" BERT model (what we saw in lecture and is described in Devlin et al. 2019)\n",
        "ner_f1_bert_base_cased_model=train_and_evaluate(\"bert-base-cased\", \"ner-f1-bert-base-cased\", embedding_size=768, num_epochs = 5, doLowerCase=False)"
      ],
      "metadata": {
        "id": "61hinGbzxFyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_sentence(ner_f1_bert_base_cased_model, \"John is from Washington, DC\")"
      ],
      "metadata": {
        "id": "Vw2ufCbs6tFJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}